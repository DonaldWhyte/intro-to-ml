<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Introduction to Machine Learning -- A Whirlwind Tour</title>

    <meta name="description"
          content="High-level introduction to machine learning and the practical process of applying it to solve industry problems">
    <meta name="author" content="Dinald Whyte">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css" id="theme">

    <link rel="stylesheet" href="css/custom.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">
      <div class="slides">
        <section>
          <section>
            <h1>Introduction to Machine Learning</h1>
            <h3>A Whirlwind Tour</h3>
            <p>
              <small>
                Created by <a href="http://donaldwhyte.co.uk">Donald Whyte</a>
                / <a href="http://twitter.com/donald_whyte">@donald_whyte</a>
              </small>
            </p>

            <div id="dft-notice">
              <p>Originally made for:</p>
              <img src="images/department-for-transport.svg"
                   alt="department for transport" />
            </div>
          </section>

          <section data-markdown>
            A bit about myself...
          </section>

          <section data-markdown data-background="images/bloomberg.jpg"
                   class="stroke large bloomberg" data-notes="
I currently work for Bloomberg as an infrastructure engineer. I help design
and build the low-level systems that keep financial data flowing to the resty
of the world.

My role is essentially a hybrid between a software engineer, architect and
data scientist. I dabble in a bit of everything basically!
">
            ![bloomberg](images/bloomberg-logo.svg)

            Infrastructure Engineer
          </section>

          <section data-markdown data-background="images/hackamena.jpg"
                   class="stroke large bloomberg" data-notes="
Have participated in, mentored at and organised 17 hackathons, across the world.

In countries such as: Egypt, UAE, Italy, Germany, the US and, of course, the UK.
">
            Hackathons

            Organiser / Mentor / Hacker
          </section>

          <section data-markdown data-notes="
My machine learning background primarily comes from the work I did at BT and IBM

I've applied machine learning to:
    * increase global network security,
    * assist organisations maintain their large, enterprise software estates
    * and more recently, I've consulted for
">
            ![bt](images/bt.svg)
            ![ibm](images/ibm.svg)
            ![helper.io](images/helper.svg)

            Applied machine learning in:

            * network security
            * enterprise software management
            * employment
          </section>
        </section>

        <section>
          <section data-markdown>
            ## What is Machine Learning?
          </section>

          <section data-markdown>
              A mechanism for machines to **learn** behaviour with no human
              intervention

              Programs that can **adapt** when exposed to new data

              Based on **pattern recognition**
          </section>

          <section data-markdown data-notes="
Machine learning has been around for a long time, but industry adoption and
academic research has grown rapidly the past five to ten years.

It's now being used in:

education
security
robotics
finance
speech recognition
image recognition
advertising

...even brewing!
">
            ![education](images/application1.jpg)
            ![security](images/application2.jpg)
            ![robotics](images/application3.jpg)
            ![finance](images/application4.jpg)
            ![speech recognition](images/application5.jpg)
            ![advertising](images/application6.jpg)
          </section>

          <section data-background="images/ml-landscape-dec15.jpg" data-notes="
Nowadays, machine learning is everywhere. You can't avoid it.

When you look on Facebook, machine learning determines what you see. When you
pay for your lunch with your credit card, machine learning decides whether the
transaction goes through, or if you're a fraud.

Global markets are affected by the machine learning algorithms hedge funds
execute.

Behind me is a tiny portion of the machine learning focused companies out there
today. ML has spurred innovation in dozens of industries, and it's slowly
toucing more and more.
">
          </section>

          <section data-markdown data-notes="
There has been a huge growth in machine learning adoption over the past few
years.
">
          ### Huge Growth
          </section>

          <section data-markdown>
          ### Why?

          * cheaper computing computer
          * cheaper data storage
          * more data than ever &dash; everyone is online
          * produces:
            - greater volume
            - greater variety
          * because it's *cool*
          </section>

          <section data-markdown data-notes="
One thing to thing before I move on is the difference between data mining,
machine learning and statistics, since I've seen people confuse these terms.

Data mining applies methods from many different areas to identify previously
unknown patterns from data. This can include statistical algorithms,
machine learning, text analytics, time series analysis and so on. Data mining
also includes the study and practice of data storage and data manipulation.

Machine learning is a *category* of data mining that uses automated and
iterative algorithms to learn patterns in data.

Machine learning focuses on algorithms that learn from data with
**minimal human intervention**.

Statistics is field of study that provides techniques used by both data mining
and machine learning, but again, it's not the same thing.

The key is that machine learning's focus is on *automating* pattern recognition,
which is what makes it different to the other two fields.
">
            ### The Difference

            ![venn diagram](images/venn-diagram.svg)
          </section>

          <section data-markdown data-notes="
To reiterate, the goal of machine learning is to automate the process of
recognising patterns in data. This way, we can build systems that adapt to
constantly changing, and unseen, data.

Machine learning techniques can mostly be broken down into four categories.

supervised learning -- predict an individual's lifespan using historical,
                       population lifetime data

unsupervised learning -- clustering humans based on their genetic makeup.
                         this is to categorise people into groups with similar
                         genetic makeup

reinforcement learning -- used widely in robotics, the learning algorithms
                          learn through trial and error, with successes and
                          failures *reinforcing* the computer's knowledge on
                          what the right output is
">
            <script type="text/template">
              ### Learning Types

              * supervised learning <!-- .element: class="fragment" data-fragment-index="1" -->
                - historical data predicts likely future events
              * unsupervised learning <!-- .element: class="fragment" data-fragment-index="2" -->
                - discover structure in unlabelled data
              * semi-supervised learning <!-- .element: class="fragment" data-fragment-index="3" -->
                - supervised learning when you don't have enough known historical data
              * reinforcement learning <!-- .element: class="fragment" data-fragment-index="4" -->
                - learn through trial and error
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            ## Supervised Learning

            Use labeled historical data to predict future outcomes
          </section>

          <section data-markdown>
            Given some input data, predict the correct output

            ![shapes](images/shapes.svg)

            What **features** of the input tell us about the output?
          </section>

          <section data-notes="
Raw input data in the shape example before would be all of the pixels that make
up a shape, or even the raw binary data that makes up the image.
">
            <h3>Feature Space</h3>

            <ul>
              <li>A feature is some property that describes raw input data</li>
              <li>
                An input can be represented as a vector in
                <strong>feature space</strong>
              </li>
              <li>2 features = 2D vector = 2D space</li>
            </ul>

            <center>
              <div id="shape-plot"></div>
            </center>
          </section>

          <section data-markdown data-notes="
Raw input data in the shape example before would be all of the pixels that make
up a shape, or even the raw binary data that makes up the image.

Other examples of raw inputs include each character in a document, each byte in
a file, every sample frequency in audio.

These inputs are highly complex and noisy, which makes it incredibly difficult
to identity patterns and infer greater meaning from the data.

So we abstract away the complexity using feature vectors, which significantly
reduce the dimensionality of the data.

* Added benefits:
  - remove redundant information
  - decreases time it takes to learn a good model
  - comprehensible by research (REFER TO PREVIOUS SLIDE)
">
            ### Why Use Feature Space?

            ![feature-extractor](images/feature-extractor.svg)

            * Could simply use raw binary data as input
            * Raw inputs are complex and noisy
            * Abstract the complexity away by using features
          </section>

          <section data-notes="
Supervised leanring depends on use having a training dataset, that has both
the input features and the *known* outputs of those feature vectors.

The training data is used to find a function, or model, that effectively
discriminates between your inputs.

The model cuts the feature space into regions, where each region
corresponds to an output class.

In the example here, we see that the model segments the feature space into
our two output classes -- triangles and squares.

Once we have a trained model, we use it to classify new, unseen data by
transforming it into a feature vector, mapping it to feature space, and
chekcing which region the vector is mapped to.
">
            <div class="left-col">
              <ul>
                <li>Training data is used to produce a model</li>
                <li> f(x&#x0304;) = mx&#x0304; + c </li>
                <li>Model divides feature space into segments</li>
                <li>Each segment corresponds to one <strong>output class</strong></li>
              </ul>
            </div>

            <div class="right-col">
              <center>
                <div id="shape-plot-discriminant"></div>
              </center>
            </div>

            <div class="clear-col"></div>

            <p>
              Use trained model to classify new, unseen inputs
            </p>
          </section>

          <section data-notes="
Now in reality, your data might not be linearly seperable, so we might have to
use a more complex model to correctly discriminate between the different output
classes.

Of course, we need to be careful our models don't overfit our input training
data, otherwise it will fail to correctly classify new, unseen data points.

We can see on the diagram on the right, there are many unseen square instances
that have been incorrectly classified as a triangle. The two triangles near them
might just be outliers, but because the model was trained on a small training
dataset, the feature space looked like it had a different structure.
">
            <h3>Choosing a Suitable Model</h3>

            <div class="left-col">
              <center>
                <div id="shape-plot-complex"></div>
              </center>
            </div>
            <div class="right-col fragment" data-fragment="1">
              <center>
                <div id="shape-plot-overfitting"></div>
              </center>
            </div>
          </section>

          <section data-markdown>
            ### Evaluation

            * Many ways to evaluate
              - overall accuracy, number of false positives, etc.
            * Depends on what problem you're trying to solve
              - are false positive acceptable?
            * `k`-fold cross validation commonly used
          </section>

          <section data-markdown data-notes="
Unless the problem you're trying to solve has specific requirements, just
using overall classification accuracy is reasonable.

However, you shouldn't just use your training dataset to evaluate the accuracy
of your classifier. As I showed previously, trained models might overfit
their training data and not classify new, unseen data points correctly.

We want classifiers that generalise and behave correctly *even* with unseen
instances. This can be achieved by splitting the training dataset into two:

* training -- used to learn the model
* testing -- used for evaluating model's accuracy on data it hasn't seen

Doing this just once means that we're not using all of our data points for
training. What if those points capture some essential structure in the data?

We can test using unseen data points and ensure all points are considered for
training by using K-FOLD CROSS VALIDATION.

With k-fold cross validation, we train a model and tests it *k* times. In each
iteration, the full dataset is split into training and test sets, but the split
is different each time.

[HIGHLIGHT DIAGRMA AND EXPLAIN EXAMPLE]

This technique is very common and is good starting point for those unfamiliar
with applying machine learning.
">
            ![k fold cross validation](images/k-fold-cross-validation.svg)

            * testing accuracy just using the training dataset is bad
            * classifier should generalise to unseen data points
            * split training data into two sets
              - one for training
              - one for testing
            * perform this process *k* times using different splits
          </section>

          <section data-markdown data-notes="
One thing to mention is that I've been focusing on classification here, instead
of regression. All of the outputs I've discussed have been from discrete, finite
sets.

You would use classification to predict what type of animal is in a picture.
Generally, you would pre-define the output classes (e.g. cat, dog, pidgeon)
and construct a model that uses those classifiers.

Supervised learning can produce models that output continuous values, in which
case we train regression models instead. If you wanted to predict the lifetime
of a human, based on some attributes about their lifestyle and current health,
then you would use regression.
">
            ### Discrete or Continuous?

            * Classification
              - output is from a finite, discrete set
              - of *'classes'*
            * Regression
              - output is a real number from a continuous range
          </section>


          <section data-markdown data-notes="
The point I'm trying to make here is that machine learning really just boils
down to three core aspects.

Extracting useful features from the input data, features that have some
correlation to the correct outputs, don't take an enourmous amount of processing
time to extract, and are not redundant (tell us the same thing as other features).
">
            <script type="text/template">
              ### So what is supervised learning?

              1. collecting a comprehensive training dataset <!-- .element: class="fragment" data-fragment-index="1" -->
              2. extracting useful features from input data <!-- .element: class="fragment" data-fragment-index="2" -->
              3. choosing a suitable model type <!-- .element: class="fragment" data-fragment-index="3" -->
              4. training the chosen model type on the training dataset <!-- .element: class="fragment" data-fragment-index="4" -->
              5. carefully evaluating model before deploying <!-- .element: class="fragment" data-fragment-index="5" -->
              6. using model to label new, unseen inputs in production <!-- .element: class="fragment" data-fragment-index="6" -->

              <div style="height: 60px"></div>

              #### Doing this in a SYSTEMATIC way! <!-- .element: class="fragment" data-fragment-index="7" -->
            </script>
          </section>

        </section>

        <section>
          <section data-markdown>
            ## Process
          </section>

          <section data-markdown data-notes="
When trying to solve a problem with machine learning, I follow a multi-step
process rigorously. After looking into how others approach machine learning
problem, I found remarkable similaraties between my methodology and Jason
Brownlee.

Jason has done a much better job of formalising these steps, so I've listed his
personal process here.

[LIST STEPS]

Some of this may seem obvious, but formalising this stuff helps. I'm sure I'm
not the only one who's found myself getting off track and pursuing a tangent
that wastes hours, or even days, of my time. Now that I always keep these steps
in mind, I've personally found myself being a lot more systematic and getting to
good solutions faster.
">
            [Jason Brownlee's Process](http://machinelearningmastery.com/process-for-working-through-machine-learning-problems/):

            1. define the problem
            2. prepare data
            3. spot check algorithms
            4. tuning
            5. present results
          </section>

          <section data-markdown>
            ### 1. Define the Problem

            Models are useless if you're solving the wrong problem

            * what is it you exactly want to do?
            * primary requirement: speed? correctness?
            * what does the data come from?
            * how will users/systems be affected by your model?
          </section>

          <section data-markdown class="large-table">
### Faces vs Vegetables

![face-veg-img](demo/data/face1.jpg)
![face-veg-img](demo/data/vegetable1.jpg)
![face-veg-img](demo/data/face5.jpg)
![face-veg-img](demo/data/vegetable10.jpg)
![face-veg-img](demo/data/vegetable19.jpg)
![face-veg-img](demo/data/vegetable7.jpg)

|             |                    |
| ----------- | ------------------ |
| **Input:**  | image (rgb pixels) |
| **Output:** | face or vegetable  |
          </section>

          <section data-markdown>
            ### 2. Prepare the Data

            Garbage in, garbage out

            * What's the source of the data?
              - collected by your system(s)?
              - provided by third-parties?
            * Which features to extract?
              - more features **&#8800;** better accuracy
              - how long does it take to extract features?
          </section>

          <section data-markdown>
            ### Data Source

            * 40 images
              - 20 faces
              - 20 vegetables
            * manually labelled by a friend
              - who we trust
          </section>

          <section data-markdown data-notes="
Use quick and easy features to extract so we can get some classification going.
">
            ### Features to Extract from Images

            * average intensity of each colour channel across all pixels
              - red, green, blue
            * average saturation across all pixels
              - bright and strong colours
          </section>

          <section data-markdown data-notes="
Previous features are summary statistics that only take colour into
consideration. Let's another feature, this time one related to the shapes in the
images.
">
            ![sobel edge detection](images/sobel-edge-detection.png)

            * how about the complexity of the images?
            * more complicated shapes have more edge pixels
            * feature:
              - proportion of pixels are 'edge pixels'
            * **sobel edge detection** used to find edge pixels
          </section>

          <section data-markdown>
            1. mean red colour
            2. mean green colour
            3. mean blue colour
            4. mean saturation
            5. edge pixel ratio
              - \# edge pixels / \# total pixels
          </section>

          <section data-markdown>
            ### 3. Spot Check Algorithms
          </section>

          <section data-markdown data-background="#fff" data-notes="
There are hundreds of machine learning algorithms out there, all training the
dozens of model types out there if varying ways.

How do you know what's going to work best with your problem's data?
">
          ![](images/machinelearningalgorithms.png)
          </section>

          <section data-background="#fff" data-notes="
First, we must understand the structure data. We can inspect the input features
ourselves and manually visualise the relation between each pair of features,
and the relation between each feature and the output class.

For example, let's suppose we want to predict the amount of frozen yoghurt
sold in one day. Suppose one of our data points is the highest temparature
during that day.

We can very clearly see linear relationship between temparature and yoghurt
sold. Hence, it makes sense to try out a linear regression model.
">
            <div id="linear-data-space-container">
              <img src="images/data-space1.png" alt="linear data space" />
            </div>
          </section>

          <section data-markdown data-background="#fff" data-notes="
Suppose we want to predict if whether or not a person attended the Burning Man
festival in Nevada. Also suppose age and salary are two of the features we've
collected. When plotting the two features against the class, we see a clear
section in the feature space where burning

Discriminating between the two classes correctly requires us to split the feature
space in a non-linear way. Multiple discriminants are required to achieve this
discrimination.

One way we can do this is segment the feature space recursively, using a
technique called decision trees.
">
            ![recursive data space](images/data-space2.png)
            ![decision tree](images/decision-tree.png)
          </section>

          <section data-markdown data-background="#fff" data-notes="
And also if we see our data has even more complicated boundaries, we can bring
out the big guns and try a neural network.

This is all well and good, but this process of manually comparing features and
classes is time consuming, especially when you have a large number of features
and many data points. Exploring and determining the structure of
high-dimensional datasets is time-consuming and complex.

We can do better than this.
">
            ![arbitrary data space](images/data-space3.png)
            ![neural network](images/neural-network.png)
          </section>

          <section data-markdown data-notes="
So how do you select which algorithms to use?

Should we always hand-pick ourselves? Other than exploring the data to find the
right models being a time-consuming process, humans are prone to bias and
mistakes.

We have a tendency to pick algorithms we've used before, or make assumptions
about the shape of the decision boundary with little basis.

Solution? Let computers choose for us...
">
            Which algorithm to use?

            * Humans can be biased
            * Take the decision out of our hands entirely
            * **Automate** the selection of algorithms
          </section>

          <section data-markdown data-notes="
By simply running the data on every algorithm you can!

Note that choosing what statistical tests to use is another topic entirely,
which we'll leave out of this talk!
">
            Spot check every algorithm you can!

            * Run your dataset(s) across dozens of algorithms
            * **10-fold cross-validation**
              - measure accuracy, false positives and false negatives
            * compare results of each algorithm using statistical tests
              - say *"algorithm A is better than B"* with confidence
          </section>

          <section data-markdown>
            <script type="text/template">
              #### End Result

              A list of all algorithms ranked by accuracy
            </script>
          </section>

          <section data-markdown data-notes="
CODE EXAMPLE
">
            ### Let's try it out!

            ![python](images/python.svg)
            ![sklearn](images/sklearn.svg)

            * Python
            * scikit-learn
            * Could also use:
              * Pylearn2, MILK, Theano, ...
          </section>

          <section data-markdown>
```
from sklearn import svm
from sklearn import tree
from sklearn import naive_bayes

classifiers = {
    'SVM':
        svm.SVC(kernel='linear', C=1),
    'Decision Tree':
        tree.DecisionTreeClassifier(criterion='gini', split='best'),
    'Gaussian Naive Bayes':
        naive_bayes.GaussianNB(),

    ...
}
```
          </section>

          <section data-markdown>
```
from sklearn import preprocessing
from sklearn import cross_validation

featureVecs = preprocessing.normalize(featureVectors)

cross_validation.cross_val_score(
        classifier, featureVectors, labels, cv=kFolds)
```
          </section>

          <section data-markdown>
| Classifier               | Mean Acc. | Confidence Interval | Lower Bound Acc |
| ------------------------ | --------- | ------------------- | --------------- |
| Gaussian Naive Bayes     | 0.975     | 0.15                | 0.825           |
| Decision Tree            | 0.95      | 0.2                 | 0.75            |
| Multi-Nomial Naive Bayes | 0.85      | 0.331662            | 0.518338        |
| SVM                      | 0.775     | 0.415331            | 0.359669        |
| Neural Network (Sigmoid) | 0.525     | 0.269258            | 0.255742        |
| Bernoulli Naive Bayes    | 0.5       | 0                   | 0.5             |
          </section>

          <section data-markdown>
            Decision tree had good performance

            ![face veg decision tree](images/face-veg-decision-tree.svg)
          </section>

          <section data-notes="
Note the cluster of vegetables on the bottom right. This is due to certain types
of vegetables, such as peppers and tomatoes. They have an very high number o
red and not much green.

The decision tree is even able to handle this second cluster because of the way
it recursively segments data space.
">
            <center>
              <div id="face-veg-colour-plot"></div>
            </center>
          </section>

          <section data-notes="
Interestly enough, there's another two features that, when used in conjunction
tell us a lot about the images.

When we plot mean saturation, the overall brightness/intesity of the image,
again the complexity of the shapes in that image, we see a cluster where all
the face images lie.

Some of the trained classifiers also picked this up, and used this clustering
in addition to the red/green features to accurately classify the images.

TAKEWAY

Of course, if you use every possible finding in the training data for your
classifier, you run the risk of overfitting and failing to generalise to new
instances.
">
            <center>
              <div id="face-veg-satedge-plot"></div>
            </center>
          </section>

          <section data-markdown>
            ### 4. Tuning

            * Pick top `x` algorithms from previous step
            * Smaller set of algorithms to manually investigate
            * Greater confidence chosen algorithms are naturally good at
              picking out the structure of the dataset / feature space
          </section>

          <section data-markdown data-notes="
ALGORITHM TUNING involves exploring the space of all possible parameter
combinations for each chosen algorithm.

ENSEMBLES are good when several algorithms have decent accuracy and the types of
inputs they struggle to classify are *different* from each other.

For example, if we have a model which accurately identifies only dogs accurately
and another that only identifies cats, you can join them in an ensemble. This
could produce a model with greater accuracy, since the weaknesses of each
individual model is handled by the other models.

FEATURE REFINEMENT is where we try diffrent features, remove some featurs,
split features into multiple features and run feature selection algorithms
to improve your feature set.

NOTE: We should use the same statistical tests that we used to rank all the
algortithms initially to ensure our tuning is truly producing more accurate
models.
">
            #### Squeeze out Remaining Performance

            1. algorithm tuning
              - tune parameters of each algorithm for better accuracy
            2. ensembles
              - combine multiple 'okay' models into one, better model
            3. feature refinement
          </section>

          <section data-markdown data-notes="
This was not an explicit step in my own process, but I like that Jason made it
explicit.

FIRST BULLET

I find presenting the final results of your research a good way of ensuring all
of your findings are recorded in some way. This information could be important
when solving similar problems in the future. If you can make the results
publicly accessible, even better! That way, other data scientists can benefit
from your research and we prevent each other repeating work someone else has
already done!

How well the solution performs on your test data, and also the limitations it
has (for example, the types of input solution struggles with).

SECOND BULLET

It is important to record everything you found along the way, whether it's
certain algorithms performing poorly, when certain tuning actions vastly
increased performed.
">
            ### 5. Present Results

            * Produce document that explains:
              - problem
              - solution (final algorithm/features/datasets used)
              - accuracy / speed of solution
              - limitations of solution
            * Be sure to list any other insights discovered along the way
          </section>
        </section>

        <section>
          <section data-markdown data-notes="
If this process is done manually, then it will take a very long time. Humans
also get impatient and are error-prone, so if one small mistake is made in the
middle, we could get invalid findings.

That's why...
">
            ![process](images/process.svg)
          </section>

          <section data-markdown data-notes="
...automation is key.

One needs to create a test harness that automates x. Automates y. Automates z.

Above all, we need to trust the harness is doing the right thing. It pays to
spend some time building this and ensuring it's right.
">
            ### Automation is Key

            * Create a test harness that:
              - feature extraction
              - trains models using many algorithms
              - evaluates models in a rigorous way
            * Ensure you can trust that harness
          </section>


          <section data-markdown data-notes="
Following this process and automating it has saved me days, weeks, maybe even
months of time.

Time that I can use to focus on the real challenge of machine learning...

...feature engineering!

Focus on feature engineering:
  * automate as much of the extraction, training and evaluation as possible
  * then spend the extra time on determining features to use!
  * feature engineering is far the most difficult/time-consuming stage
  * I cannot stress that enough
">
            <script type="text/template">
              Automating this process allows me to focus on what real challenge...

              ## FEATURE ENGINEERING <!-- .element: class="fragment" data-fragment-index="1" -->
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            ## Summary
          </section>

          <section data-markdown>
            Machine learning is all about **automation**.

            It's about automatically finding patterns in data...

            ...and building models to fit that data.

            Models that also *generalise*.
          </section>

          <section data-markdown>
            <script type="text/template">
              Use of machine learning is **growing faster than ever**.

              Soon it will be *ubiquitous*.

              (it might already be there...) <!-- .element: class="small" -->
            </script>
          </section>

          <section data-markdown>
            Follow the five step process:

            1. define problem
            2. prepare data
            3.  spot check algorithms
            4. tuning
            5. present results

            Automate as much as you can.

            So you can focus on **feature engineering**.
          </section>

          <section data-markdown>
            Don't reinvent the wheel.

            Use the **hundreds of tools** already there.
          </section>

          <section data-markdown data-notes="
I'm sure you all know R, which is currently the de-facto standard when it comes
to machine learning.

Python, Java and Scala are also very strong alternatives
Much easier to build automated systems that integrate

Some popular Ml libraries for Python are sklearn and Theano
Theano is very good if you're inteested in building fast neural networks
MDP is another Python ML library which makes it very easy for you to extend and
your own training algorithms

Apache Spark
Machine learning distributed across hundres of machines
Good when your data is very, very large
Requires strong software engineering background to get up and running though
">
            ### Tools

            ![r](images/r.svg)
            ![python](images/python.svg)
            ![java](images/java.svg)
            ![scala](images/scala.svg)

            ![sklearn](images/sklearn.svg)
            ![theano](images/theano.png)
            ![mdp](images/mdp.png)
            ![spark](images/spark.png)
          </section>

          <section data-markdown data-notes="
DATA MINING:
  * Book
  * Introduction to applied machine learning (forget the mention of data mining in the title).
  * Focus on the algorithms and on the process of applied machine learning.
  * 100 pages dedicated to the companion platform for applied machine learning called Weka.

If you want to focus on the process and use a mature graphical tool, I highly recommend this book.

If you want to lighter reads, then I recommend these web resources:

TOUR OF ML ALGORITHMS:
  * An easy to understand overview of the different types of machine learning
  * explains all the common algorithsm
  * and when may want to use them

BROWNLEE'S PROCESS:
  * High-level discussion on the overall process of applying machine learning
  * in an efficient and automated way

INTRO TO ML W/ SKLEARN:
  * more practical
  * softly introduces some of the concepts of machine learning
  * by letting you build your own classifiers using Python and scikit-learn

">
            ### Useful Resources

            [Data Mining: Practical Machine Learning Tools and Techniques](http://machinelearningmastery.com/6-practical-books-for-beginning-machine-learning/)

            [A Tour of Machine Learning Algorithms](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)

            [Jason Brownlee's Process](http://machinelearningmastery.com/process-for-working-through-machine-learning-problems/)

            [Introduction to Machine Learning with sci-kit Learn](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)
          </section>

          <section data-markdown>
            ### This Presentation

            [donaldwhyte.github.io/intro-to-ml](http://donaldwhyte.github.io/intro-to-ml)

            [github.com/DonaldWhyte/intro-to-ml](http://github.com/DonaldWhyte/intro-to-ml)

          </section>

          <section>
            <h3>Get in Touch</h3>

            <div class="left-col">
              <div class="donald"></div>
            </div>
            <div class="right-col">
              <div class="contact-details">
                <a href="mailto:donaldwhyte0@gmail.com">donaldwhyte0@gmail.com</a>
                <a href="ttp://twitter.com/donald_whyte">@donald_whyte</a>
                <a href="http://github.com/DonaldWhyte">http://github.com/DonaldWhyte</a>
              </div>
            </div>
          </section>

          <section data-background="images/stb-background.jpg"
                   class="stroke large"
                   data notes="
I also love hackathons, and have attended, mentored and helped organise almost
20 hackathons across the world.

And actually mentored and helped many particpants apply machine learning to
solve their target problems.

Currently, I'm co-organiser of StartupBus Europe, a cross-country hackathon on
wheels where seven buses from different countries build products and companies
on the road.

We're still accepting applications if people want to get on board. Also, if you
know an organisation or individual who'd like to get involved, help out or
sponsor, then get in touch!
">
            <img src="images/startupbus-logo.svg" alt="StartupBus UK" />

            <p>uk@startupbus.com</p>
          </section>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script src="lib/js/jquery-latest.min.js"></script>
    <script src="lib/js/highcharts.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

      function loadChartEvent(event) {
        // Remove Highcharts.com marker
        $("text:contains('Highcharts.com')").remove();
      }

      var shapeChart = {
        chart: {
          type: 'scatter',
          zoomType: 'xy',
          backgroundColor: 'rgba(0, 0, 0, 0)',
          events: {
            load: loadChartEvent
          }
        },
        title: {
          text: ''
        },
        xAxis: {
          title: {
            text: 'Area',
            style: {
              fontSize: '24px',
              color: 'white',
              'font-weight': 'strong'
            }
          },
          labels: {
            style: {
              color: 'white',
              'font-weight': 'strong',
              'font-size': '14px'
            }
          }
        },
        yAxis: {
          title: {
            text: 'Perimeter',
            style: {
              fontSize: '24px',
              color: 'white'
            }
          },
          labels: {
            style: {
              color: 'white',
              'font-weight': 'strong',
              'font-size': '14px'
            }
          }
        },
        legend: {
          enabled: false
        },
        tooltip: {
          enabled: false
        },
        exporting: {
          enabled: false
        },
        plotOptions: {
          scatter: {
            marker: {
              radius: 10,
              states: {
                hover: {
                  enabled: true,
                  lineColor: '#000'
                }
              }
            }
          }
        },
        series: [
          {
            name: 'Square',
            marker: {
              symbol: 'square'
            },
            color: 'rgba(223, 83, 83, 1)',
            data: [
              [3.7, 0.9], [3.9, 1.7],
              [3.9, 0.9], [4.2, 1.2],
              [3.5, 1.4], [3.85, 1.3],
              [3.4, 1.0], [3.7, 1.25]
            ]
          }, {
            name: 'Triangle',
            marker: {
              symbol: 'triangle'
            },
            color: 'rgba(119, 152, 191, 1)',
            data: [
              [2.75, 0.18], [3.2, 0.22],
              [4.2, 0.65], [4.1, 0.20],
              [3.5, 0.4], [4.0, 0.24],
              [4.15, 0.5], [3.8, 0.5],
              [2.8, 0.6], [3.8, 0.5],
              [3.0, 0.8], [2.7, 1.4],
              [3.1, 0.5], [2.8, 1.3],
              [2.7, 1.1], [2.8, 0.9],
              [3.15, 1.25], [3.5, 1.9]
            ]
          }
        ]
      };

      var shapeChartWithDiscriminant = jQuery.extend(true, {}, shapeChart);
      shapeChartWithDiscriminant["series"].push({
        type: 'line',
        name: 'Discriminant',
        data: [[4.2, 0.1], [2.8, 1.9]],
        marker: {
          enabled: false
        },
        enableMouseTracking: false,
        showInLegend: false,
        lineWidth: 5,
        lineColor: 'white'
      });

      var shapeChartComplex = jQuery.extend(true, {}, shapeChart);
      shapeChartComplex['series'].push({
        type: 'spline',
        name: 'Discriminant',
        data: [[3.65, 1.9], [3.2, 1.0], [3.5, 0.65], [4.2, 0.8]],
        marker: {
          enabled: false
        },
        enableMouseTracking: false,
        showInLegend: false,
        lineWidth: 5,
        lineColor: 'white'
      });

      var shapeChartOverfitting = jQuery.extend(true, {}, shapeChartComplex);
      shapeChartOverfitting['series'].push({
        name: 'Square-Wrong',
        marker: {
          symbol: 'square'
        },
        color: '#0f0',
        showInLegend: false,
        data: [ [3.35, 1.75], [3.2, 1.7], [3.4, 1.5], [3.1, 1.5], [3.3, 1.55] ]
      });

      var faceVegChartBase = jQuery.extend(true, {}, shapeChart);
      faceVegChartBase.legend = {
      layout: 'vertical',
            align: 'right',
            verticalAlign: 'top',
            floating: true,
            backgroundColor: '#333',
            borderWidth: 2,
            borderColor: 'white',
            itemStyle: {
              color: 'white',
              fontWeight: 'bold',
              fontSize: '20px'
            }
      };

      var faceVegColourChart = jQuery.extend(true, {}, faceVegChartBase);
      faceVegColourChart.xAxis.title.text = 'Mean Red';
      faceVegColourChart.yAxis.title.text = 'Mean Green';
      faceVegColourChart.yAxis.min = 0.0;
      faceVegColourChart.yAxis.max = 1.0;


      faceVegColourChart.series = [
          {
            name: 'Vegetable',
            color: 'green',
            data: [
[0.62825796545311896, 0.67994890219101178], [0.44222595103724671, 0.8204201249170765], [0.58531128491113704, 0.60324521170415235], [0.58041854550606531, 0.63824607479961559], [0.57659280234518728, 0.69729520178836824], [0.59159659922631813, 0.74551419390888884], [0.52946670196963042, 0.71172928052486994], [0.56342968877925348, 0.69473812318093642], [0.77320474934562655, 0.55440175620347742], [0.78059456270452332, 0.50696818313432157], [0.57516649303966305, 0.60508085282821944], [0.8092456798311054, 0.52960216894278223], [0.82423365222562373, 0.53526313538121428], [0.5923025309029224, 0.71851102768489361], [0.5349588901652591, 0.7084497463999504], [0.98126442717469198, 0.17893584103921623], [0.54883891504355209, 0.67890105313957672], [0.58414058069203412, 0.59291046684661053], [0.58717648603601202, 0.65362052267769277], [0.64072049500710948, 0.73107086895513762]
            ]
          },
          {
            name: 'Face',
            color: 'salmon',
            data: [
[0.64335456631161725, 0.54833413478957149], [0.63075130927072298, 0.53215338327208184], [0.61661732290582316, 0.50703616089961345], [0.68019031675911112, 0.53765909173164961], [0.63344089631648659, 0.54435119942158561], [0.67317735733202178, 0.56239550040752406], [0.72591151185357672, 0.5101929505771392], [0.68021794608340902, 0.50821770957406698], [0.63522730707207986, 0.54483388583319037], [0.69515494597691074, 0.52374099610613856], [0.66412488972527173, 0.52254823355393953], [0.73835246987439807, 0.50618471553055666], [0.71149167193162177, 0.53260161457261246], [0.65438260743552135, 0.54661702286662961], [0.70846258497049652, 0.52417427718724108], [0.68986844862130614, 0.52783535697019135], [0.6377252687547682, 0.55152096316633892], [0.64838484088639647, 0.56238751244074292], [0.69852226357237435, 0.546157630766407], [0.66608450315468415, 0.53830853641149046]
            ]
          },
          {
            type: 'line',
            name: 'Discriminant',
            data: [[0.4, 0.5777], [1.0, 0.5777]],
            marker: {
              enabled: false
            },
            enableMouseTracking: false,
            showInLegend: false,
            lineWidth: 5,
            lineColor: '#0f0'
          },
          {
            type: 'line',
            name: 'Discriminant',
            data: [[0.7558, 0.0], [0.7558, 1.0]],
            marker: {
              enabled: false
            },
            enableMouseTracking: false,
            showInLegend: false,
            lineWidth: 5,
            lineColor: 'red'
          }
      ];

      var faceVegSatEdgeChart = jQuery.extend(true, {}, faceVegChartBase);
      faceVegSatEdgeChart.xAxis.title.text = 'Mean Saturation';
      faceVegSatEdgeChart.yAxis.title.text = 'Edge Pixel Ratio';
      faceVegSatEdgeChart.series = [
          {
            name: 'Vegetable',
            color: 'green',
            data: [
[0.59593200000000002, 0.67122499999999996], [0.70201499999999994, 0.57808000000000004], [0.17555899999999999, 0.42407400000000001], [0.322714, 0.55174500000000004], [0.53980799999999995, 0.76991399999999999], [0.65932000000000002, 0.81395300000000004], [0.417043, 0.86703300000000005], [0.42941299999999999, 0.77066400000000002], [0.654586, 0.54507499999999998], [0.59687400000000002, 0.81956200000000001], [0.234067, 0.35811199999999999], [0.75917800000000002, 0.55525400000000003], [0.77804700000000004, 0.48641800000000002], [0.59282800000000002, 0.75765300000000002], [0.48323199999999999, 0.18110200000000001], [0.93068700000000004, 0.15890799999999999], [0.35638999999999998, 0.63439999999999996], [0.119183, 0.10445699999999999], [0.387656, 0.52646599999999999], [0.656169, 0.48806699999999997]
            ]
          },
          {
            name: 'Face',
            color: 'salmon',
            data: [
[0.209062, 0.15003], [0.20863599999999999, 0.38819399999999998], [0.21088000000000001, 0.33875499999999997], [0.28626299999999999, 0.26505800000000002], [0.18093100000000001, 0.389428], [0.29617100000000002, 0.40819699999999998], [0.37652799999999997, 0.122555], [0.31248500000000001, 0.16249], [0.19806499999999999, 0.34207700000000002], [0.316467, 0.21126800000000001], [0.25652000000000003, 0.24348800000000001], [0.43935099999999999, 0.43424499999999999], [0.38750400000000002, 0.33570800000000001], [0.25747999999999999, 0.40101100000000001], [0.35791299999999998, 0.27700000000000002], [0.30728699999999998, 0.10234699999999999], [0.22576599999999999, 0.146427], [0.26536599999999999, 0.29931799999999997], [0.38230500000000001, 0.312276], [0.239536, 0.366562]
            ]
          }
      ];

      $(function () {
        $('#shape-plot').highcharts(shapeChart);
        $('#shape-plot-discriminant').highcharts(shapeChartWithDiscriminant);
        $('#shape-plot-complex').highcharts(shapeChartComplex);
        $('#shape-plot-overfitting').highcharts(shapeChartOverfitting);
        $('#face-veg-colour-plot').highcharts(faceVegColourChart);
        $('#face-veg-satedge-plot').highcharts(faceVegSatEdgeChart);
      });
    </script>

  </body>
</html>
